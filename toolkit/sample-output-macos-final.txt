[0;32m
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Docker Performance & Security Analysis Toolkit           â•‘
â•‘   Deep-dive companion for container optimization           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[0m

\033[0;34mWhat this toolkit does:\033[0m
Exposes how Linux kernel primitives (namespaces, cgroups, OverlayFS)
shape container performance, security, and operational characteristics.

\033[0;34mWhat this toolkit is NOT:\033[0m
â€¢ NOT a storage benchmark suite (use fio/iozone for that)
â€¢ NOT a comprehensive security scanner (use trivy/grype for that)
â€¢ NOT a production monitoring tool (use Prometheus/Datadog for that)

\033[0;34mUse this to:\033[0m
â€¢ Understand platform-specific container behavior
â€¢ Establish baseline performance characteristics
â€¢ Validate that Docker primitives work as expected
â€¢ Identify optimization opportunities (volumes vs OverlayFS, etc.)


[0;34m========================================[0m
[0;34mChecking Prerequisites[0m
[0;34m========================================[0m

[0;32mâœ“[0m docker found: Docker version 28.4.0, build d8eb465
[1;33mâš [0m strace not found (optional for syscall tracing)
[1;33mâš [0m perf not found (optional for CPU profiling)
[0;32mâœ“[0m jq found
[1;33mâš [0m bpftrace not found (optional for eBPF tracing)

[0;34m========================================[0m
[0;34mTest 1: Container Startup Latency[0m
[0;34m========================================[0m

Untagged: alpine:latest
Deleted: sha256:865b95f46d98cf867a156fe4a135ad3fe50d2056aa3f25ed31662dff6da4eb62
=== Phase 1: Image Pull (network + extraction) ===
Image pull time: 2308ms

=== Phase 2: Container Runtime (namespace + exec) ===
Measuring first run (cold - may include overlay setup)...
Cold start (no pull): 449ms

Measuring warm start (cached layers)...
Warm start (cached): 371ms

=== Phase 3: Average Runtime (10 iterations) ===
Average startup time: 303ms

=== Decomposition Summary ===
Pull + extraction: 2308ms (network + registry)
Runtime overhead:  303ms (namespace + exec)
Total cold start:  2757ms

The 303ms runtime overhead reflects Docker's architecture on this platform.
This includes: namespace creation (single-digit milliseconds), OverlayFS mount,
cgroup setup (low-millisecond range), and platform-specific operations
(containerd shim initialization, managed disk metadata access).
On bare metal or optimized runtimes, total overhead is typically 100-200ms.
Runtime: 303ms
Note: Container startup includes namespace creation, OverlayFS mount,
cgroup setup, and storage I/O. Cloud environments typically show 300-700ms
due to the combination of kernel operations and storage layer interactions.

[0;34m========================================[0m
[0;34mTest 2: Container Process Hierarchy & Syscall Overview[0m
[0;34m========================================[0m

[1;33mâš [0m strace not available, skipping

[0;34m========================================[0m
[0;34mTest 3: OverlayFS Layer Analysis[0m
[0;34m========================================[0m

Pulling multi-layer image (nginx)...

Image layers:
"sha256:1869137e3b746de54b735253d5641cdc70b86dbab8a26b9aafcd5faaecc25a8f"
"sha256:d541d4dbb4431d2e774b15bb5800566f8d12b8625266d159d2aad6be7a419a4f"
"sha256:59db3a4886a61f1679dfe3ee31f63a9bf7fb609e399dc8d1cac5580cd80725c7"
"sha256:f51dbcf623b14143c34f047e6cbb33b7eb86038a3243f3ad484c5dbb93afb570"
"sha256:a39c4f7ad9933448a2dd67536cdd7700a69fa839986c2d99afe37210f03cf08c"
"sha256:ec7ae0ddda062f7f4dc8d64eba3d0c084e0c69289b03e6a1f5d9c732deadfb51"
"sha256:1edb484b672426fa0cc3c59283a733721a52d08dc66f1ddbe67c7be2d6c36004"
"sha256:57f5fe10a37b9e59b45349e107f01c2d6d1d7fdea50ed685d35d3b24873d6d47"

Physical layer storage:
Image ID: 8491795299c8e739b7fcc6285d531d9812ce2666e07bd3dd8db00020ad132295
[1;33mâš [0m /var/lib/docker/overlay2 not accessible (may require root)

[0;34m========================================[0m
[0;34mTest 4: I/O Performance Analysis[0m
[0;34m========================================[0m

=== Sequential Write Performance ===
Testing container filesystem (OverlayFS upper layer)...
104857600 bytes (100.0MB) copied, 0.054166 seconds, 1.8GB/s

[1;33mNote:[0m Volume mount test skipped on macOS Docker Desktop due to
filesystem compatibility issues. Volume performance on macOS varies significantly
based on Docker Desktop version and APFS mount optimizations.

=== OverlayFS Copy-up Overhead ===
Creating container with pre-existing 100MB file...
-rw-r--r--    1 root     root      100.0M Dec 30 19:03 /bigfile
Triggering copy-up by modifying file in read-only layer...
Copy-up operation time: 88ms

[0;32mAnalysis:[0m Copy-up overhead <100ms - acceptable for this file size.

[0;34m========================================[0m
[0;34mTest 5: Network Performance[0m
[0;34m========================================[0m

=== Network Connectivity Verification ===
Testing bridge network connectivity...
10 packets transmitted, 10 packets received, 0% packet loss

Testing host network connectivity...
10 packets transmitted, 10 packets received, 0% packet loss

[1;33mNote:[0m This is a connectivity sanity check, not a latency benchmark.
For precise network performance analysis, use tools like iperf3 or netperf.

Network mode characteristics:
â€¢ Bridge mode: Adds veth pair + iptables NAT
  (typical overhead: ~0.1-0.3ms based on kernel networking behavior)
â€¢ Host mode: Direct host network stack (minimal overhead)

Recommendations:
â€¢ For latency-critical services: Consider host networking
â€¢ For isolation and multi-tenancy: Use bridge networking (standard)

[0;34m========================================[0m
[0;34mTest 6: Memory Efficiency & Page Cache Sharing[0m
[0;34m========================================[0m

Starting 3 identical nginx containers...

Individual container memory usage:
NAME      MEM USAGE / LIMIT     MEM %
nginx1    8.078MiB / 9.704GiB   0.08%
nginx2    7.309MiB / 9.704GiB   0.07%
nginx3    7.305MiB / 9.704GiB   0.07%

Physical memory (RSS) per container:
nginx1 (PID 4034407): N/A KB
nginx2 (PID 4034474): N/A KB
nginx3 (PID 4034559): N/A KB

Note: Shared pages (like nginx binary) are counted once in physical memory
Total reported may exceed actual RAM usage due to page cache sharing

[0;34m========================================[0m
[0;34mTest 7: Security Posture Analysis[0m
[0;34m========================================[0m

Checking default container capabilities...
CapInh:	0000000000000000
CapPrm:	00000000a80425fb
CapEff:	00000000a80425fb
CapBnd:	00000000a80425fb
CapAmb:	0000000000000000

Capability names (requires libcap):
Install libcap2-bin to decode capabilities

Testing restricted container (no capabilities):
CapInh:	0000000000000000
CapPrm:	0000000000000000
CapEff:	0000000000000000
CapBnd:	0000000000000000
CapAmb:	0000000000000000

Checking for privileged containers (security risk):
[0;32mâœ“[0m No privileged containers running

Checking Docker socket mounts (security risk):
[0;32mâœ“[0m No containers with Docker socket access

[0;34m========================================[0m
[0;34mTest 8: CPU Performance & Throttling[0m
[0;34m========================================[0m

=== CPU Usage Without Limits ===
Starting CPU-intensive container (infinite loop)...
CPU usage:
NAME       CPU %
cpu-test   100.01%

=== CPU Throttling Test (50% of 1 core) ===
Starting CPU-limited container...
CPU usage (target: 50.00%):
Measured: 50.32%
Throttling accuracy: 100%
[0;32mâœ“[0m Excellent cgroup enforcement (<1% variance)

[0;32mAnalysis:[0m CPU cgroups provide deterministic resource isolation.
This is a kernel-level guarantee, platform-invariant across infrastructure.

[0;34m========================================[0m
[0;34mTest 9: Namespace Isolation Inspection[0m
[0;34m========================================[0m

Starting test container...
Container PID: 4035001

Namespace links for container process:
Cannot access namespaces (requires root)

Comparing to host namespaces:
Host PID namespace:
Requires root
Container PID namespace:
Requires root

Container's view of processes (should only see itself):
PID   USER     TIME  COMMAND
    1 root      0:00 sleep 60
    7 root      0:00 ps aux

[0;34m========================================[0m
[0;34mTest 10: eBPF-based Syscall Tracing (Advanced)[0m
[0;34m========================================[0m

[1;33mâš [0m bpftrace not available, skipping

[0;34m========================================[0m
[0;34mPerformance Analysis Summary[0m
[0;34m========================================[0m

Docker daemon info:
 Storage Driver: overlayfs
 Kernel Version: 6.10.14-linuxkit
 Operating System: Docker Desktop
 CPUs: 8
 Total Memory: 9.704GiB

[0;32mAnalysis complete![0m

Key findings:
1. Startup latency varies by environment:
   â€¢ Local development (NVMe SSD, warm cache): 100-200ms typical
   â€¢ Cloud Premium SSD: 200-500ms typical
   â€¢ Cloud Standard HDD: 500-800ms typical
   Your environment determines expectations.
2. OverlayFS copy-up operations add latency for large file modifications
3. Use volumes for write-heavy workloads to bypass storage driver
4. Host networking reduces latency by ~0.2ms but sacrifices isolation
5. Page cache sharing makes multiple identical containers memory-efficient
6. Avoid privileged containers and Docker socket mounts in production

[1;33mRecommendations:[0m
â€¢ Use multi-stage builds to reduce image size
â€¢ Minimize layer count (combine RUN commands)
â€¢ Drop unnecessary capabilities (--cap-drop=ALL)
â€¢ Set resource limits (--memory, --cpus) for production
â€¢ Use read-only root filesystem where possible (--read-only)
â€¢ Enable user namespace remapping for additional security

[0;32mAll tests completed![0m
Logs and traces saved in /tmp/
